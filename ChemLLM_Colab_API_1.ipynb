{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패키지 설치\n",
    "!pip install transformers accelerate torch flask pyngrok nest-asyncio -q\n",
    "print('[OK] Packages installed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from google.colab import userdata\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen2-1.5B-Instruct\"\n",
    "\n",
    "print(f\"Loading model: {MODEL_NAME}...\")\n",
    "\n",
    "try:\n",
    "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "    print(\"[OK] HuggingFace token loaded from secrets\")\n",
    "except:\n",
    "    HF_TOKEN = \"YOUR_HF_TOKEN_HERE\"\n",
    "    print(\"[WARNING] Using hardcoded token\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    token=HF_TOKEN,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    token=HF_TOKEN,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"[OK] Model loaded on {model.device}\")\n",
    "print(f\"[OK] Model: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from flask import Flask, request, jsonify\nimport traceback\n\napp = Flask(__name__)\n\n\n@app.route('/health', methods=['GET'])\ndef health():\n    return jsonify({\n        \"status\": \"healthy\",\n        \"model\": MODEL_NAME\n    })\n\n\n@app.route('/summarize', methods=['POST'])\ndef summarize_analysis():\n    \"\"\"\n    CAMEO analysis result summary in ENGLISH\n    (Translation will be handled by external API)\n    \"\"\"\n    try:\n        print(\"=\" * 70)\n        print(\"[Summarize] New request\")\n        print(\"=\" * 70)\n\n        data = request.get_json()\n\n        if not data or \"analysis\" not in data:\n            return jsonify({\n                \"success\": False,\n                \"error\": \"Missing 'analysis' field\"\n            }), 400\n\n        analysis = data[\"analysis\"]\n        summary = analysis.get(\"summary\", {})\n        dangerous_pairs = analysis.get(\"dangerous_pairs\", [])\n        caution_pairs = analysis.get(\"caution_pairs\", [])\n\n        print(f\"[Summarize] Received: {summary.get('total_pairs', 0)} pairs\")\n\n        # STRONG English-only prompt\n        prompt = f\"\"\"You are a chemical safety expert. Write ONLY IN ENGLISH. DO NOT use Korean or any other language.\n\nAnalysis Results:\n- Total combinations tested: {summary.get('total_pairs', 0)}\n- Dangerous combinations: {summary.get('dangerous_count', 0)}\n- Caution required: {summary.get('caution_count', 0)}\n- Safe combinations: {summary.get('safe_count', 0)}\n- Overall status: {summary.get('overall_status', 'Unknown')}\n\n\"\"\"\n\n        if dangerous_pairs:\n            prompt += \"\\nDangerous Combinations Found:\\n\"\n            for i, pair in enumerate(dangerous_pairs[:3], 1):\n                chem1 = pair.get('chemical_1', 'Unknown')\n                chem2 = pair.get('chemical_2', 'Unknown')\n                prompt += f\"\\n{i}. {chem1} + {chem2}\\n\"\n                prompt += f\"   Status: {pair.get('status', 'Unknown')}\\n\"\n                prompt += f\"   Severity Score: {pair.get('severity_score', 0)}/20\\n\"\n                hazards = pair.get('hazards', [])\n                if hazards:\n                    prompt += f\"   Key Hazards: {hazards[0][:100]}...\\n\"\n\n        if caution_pairs:\n            prompt += f\"\\nCaution Combinations: {len(caution_pairs)} found\\n\"\n\n        if summary.get('safe_count', 0) > 0:\n            prompt += f\"\\nSafe Combinations: {summary.get('safe_count')} found\\n\"\n\n        prompt += \"\"\"\nIMPORTANT: Write your response ONLY IN ENGLISH. Do not use Korean.\n\nWrite a clear, professional summary in English (3-5 sentences) covering:\n1. Overall risk assessment\n2. Main hazards and specific dangerous combinations\n3. Critical safety recommendations\n\nUse proper chemical terminology. Be concise and accurate.\n\nResponse (ENGLISH ONLY):\"\"\"\n\n        print(f\"[Summarize] Generating English summary...\")\n\n        messages = [{\"role\": \"user\", \"content\": prompt}]\n\n        prompt_text = tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True\n        )\n\n        inputs = tokenizer(\n            prompt_text,\n            return_tensors=\"pt\",\n            truncation=True,\n            max_length=1536\n        )\n\n        if model.device.type != \"cpu\":\n            inputs = {k: v.to(model.device) for k, v in inputs.items()}\n\n        with torch.no_grad():\n            outputs = model.generate(\n                **inputs,\n                max_new_tokens=400,\n                temperature=0.3,\n                top_p=0.9,\n                do_sample=True,\n                pad_token_id=tokenizer.pad_token_id,\n                eos_token_id=tokenizer.eos_token_id\n            )\n\n        result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n        # Remove prompt - find where the actual response starts\n        markers = [\n            \"Response (ENGLISH ONLY):\",\n            \"Be concise and accurate.\",\n            \"assistant\",\n            \"Assistant:\",\n            \"<|im_start|>assistant\",\n            \"ENGLISH ONLY):\"\n        ]\n\n        for marker in markers:\n            if marker in result:\n                result = result.split(marker)[-1].strip()\n\n        # Clean up\n        result = result.strip(\". \\t\\n\")\n\n        # Remove empty lines - FIXED: use \\n not empty string\n        lines = [line.strip() for line in result.split(\"\\n\") if line.strip()]\n        result = \" \".join(lines)\n\n        # Remove any Korean characters (fallback safety)\n        import re\n        korean_pattern = re.compile('[가-힣]+')\n        if korean_pattern.search(result):\n            print(\"[WARNING] Korean detected in output, attempting to filter...\")\n            # If Korean is found, try to extract only English portions\n            sentences = result.split('.')\n            english_sentences = []\n            for sent in sentences:\n                if not korean_pattern.search(sent):\n                    english_sentences.append(sent)\n            if english_sentences:\n                result = '. '.join(english_sentences).strip() + '.'\n\n        print(f\"[Summarize] Generated: {len(result)} chars (English)\")\n        print(f\"[Summarize] Preview: {result[:100]}...\")\n\n        return jsonify({\n            \"success\": True,\n            \"summary\": result,\n            \"language\": \"en\"\n        })\n\n    except Exception as e:\n        print(f\"[ERROR] {str(e)}\")\n        traceback.print_exc()\n\n        return jsonify({\n            \"success\": False,\n            \"error\": str(e),\n            \"traceback\": traceback.format_exc()\n        }), 500\n\n\nprint(\"[OK] Flask app configured\")\nprint(\"[OK] Endpoints: GET /health, POST /summarize (English output)\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyngrok import ngrok\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "NGROK_AUTH_TOKEN = \"34dflI9kRYLX8COEWV7CxYAAQMA_W7dBiZTCfp6oe3Lf1LTY\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Starting ngrok tunnel...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "try:\n",
    "    ngrok.kill()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
    "    public_url = ngrok.connect(5000, bind_tls=True)\n",
    "    \n",
    "    if hasattr(public_url, 'public_url'):\n",
    "        tunnel_url = public_url.public_url\n",
    "    else:\n",
    "        tunnel_url = str(public_url)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"[OK] API Server Ready!\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\nPublic URL: {tunnel_url}\")\n",
    "    print(f\"\\n사용 방법:\")\n",
    "    print(f\"  1. 위 URL 복사\")\n",
    "    print(f\"  2. 로컬 .env 파일에 추가:\")\n",
    "    print(f\"     COLAB_API_URL={tunnel_url}\")\n",
    "    print(f\"\\n테스트:\")\n",
    "    print(f\"  curl {tunnel_url}/health\")\n",
    "    print(\"\\n\" + \"=\" * 70 + \"\\n\")\n",
    "    \n",
    "    print(\"Starting Flask server on port 5000...\")\n",
    "    print(\"Endpoints:\")\n",
    "    print(\"  GET  /health\")\n",
    "    print(\"  POST /summarize\")\n",
    "    print(\"=\" * 70 + \"\\n\")\n",
    "    \n",
    "    app.run(port=5000)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n[ERROR] {e}\")\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}